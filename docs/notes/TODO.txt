Done:
- Generate Dataset: Real spectra vs synthitic spectra
- Create Dataloader for spectra
- Create original CycleGAN for this data type
- Add TTUR
- Fix output of Discriminator (Reshape + Linear Layer)
- Add Wasserstein Loss + Spectral Normalization
    - Use a linear activation function in the output layer of the critic model (instead of sigmoid).
    - Use Wasserstein loss to train the critic and generator models that promote larger difference between scores for real and generated images.
    - Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).
    OR
    - Gradient Penalty
    - Update the critic model more times than the generator each iteration (e.g. 5).
- Setup Random Forest code
- Improve Loss visualization
- Spectral Normalization
- Convert matlab code to python (pydicom)
- Improve dicom2matlab code
- Fix loss

TODO:
- Add Perceptual Feature Loss
- LCModel: Preprocess parameters so that cre val is 1 and everyhing is relative to it
- Parameter search https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html

1) Create Baselines
    - synthetic - synthetic
    - synthetic - real
    - real - real


2) Validation:
    - Train CycleGAN
    - Generate synthetic looking spectra from real spectra
    - Train RF on synthetic spectra
    - Validate RF on transformed real data

3) Cross validate Baselines with results


When all losses are almost 0, that is because the Discriminator is confused. He produces almost the same output for both real and fake images. Hence, the distance between the scores is 0.
This also means that the Generator can easily fool the Discriminator and thus has a low loss.

Ideally the Discriminator should be able to tell them easily apart up to the point where the Generator has become so good that there really is no more difference.



/home/kreitnerl/mrs-gan/train.py --dataroot ./datasets/spectra --name spectra_cyclegan --model cycleGAN_WGP --no_dropout --input_nc 1 --output_nc 1 --dataset_mode dicom_spectral_dataset --gpu_ids 5 --which_model_netG resnet_6blocks --ngf 32 --ndf 32 --TTUR --which_model_netD spectra --real --batch_size 200 --n_critic 5 --gan_mode wgangp --dlr 0.001 --beta1 0 --beta2 0 --n_epochs_dis_decay 30


load spectra from: /home/kreitnerl/Datasets/Synthetic_data/dataset_magnitude.mat
load parameters from: /home/kreitnerl/Datasets/Synthetic_data/dataset_parameters.mat
training random forest with 100 trees on 90000 data samples...
training completed in 12956.396 sec
storing random forest weights at ./results/RF_2020-10-23_23-34-52_515575.joblib
prediction of 10000 samples completed in 1.212 sec
Average Relative Error cho: 0.02132080474039676

Average Relative Error naa: 0.03763041926932573